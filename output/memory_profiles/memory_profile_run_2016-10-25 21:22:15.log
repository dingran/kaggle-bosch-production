Filename: explore2.py

Line #    Mem usage    Increment   Line Contents
================================================
   286    170.1 MiB      0.0 MiB   @profile(stream=f_mem)
   287                             def main(run_info_fname=None,
   288                                      N_start=None,
   289                                      N_files_train=10,
   290                                      N_files_test=10,
   291                                      cv=False,
   292                                      # if True running cross validation if False, run single model training session and importance analysis
   293                                      feature_down_select=False,
   294                                      analyze_feature_importance=False,
   295                                      early_stop_rounds=10
   296                                      ):
   297    170.1 MiB      0.0 MiB       datetime_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
   298                             
   299    170.1 MiB      0.0 MiB       if run_info_fname is None:
   300    170.1 MiB      0.0 MiB           run_info = dict()
   301    170.1 MiB      0.0 MiB           N_splits = ub.N_split
   302    170.1 MiB      0.0 MiB           if N_files_train > N_splits:
   303                                         N_files_train = N_splits
   304    170.1 MiB      0.0 MiB           if N_files_test > N_splits:
   305                                         N_files_test = N_splits
   306                             
   307    170.1 MiB      0.0 MiB           if analyze_feature_importance and feature_down_select:
   308                                         assert 0
   309                             
   310    170.1 MiB      0.0 MiB           run_info['N_splits'] = N_splits
   311    170.1 MiB      0.0 MiB           run_info['N_files_train'] = N_files_train
   312    170.1 MiB      0.0 MiB           run_info['N_files_test'] = N_files_test
   313    170.1 MiB      0.0 MiB           run_info['feature_down_select'] = feature_down_select
   314    170.1 MiB      0.0 MiB           run_info['cv'] = cv
   315    170.1 MiB      0.0 MiB           run_info['analyze_feature_importance'] = analyze_feature_importance
   316    170.1 MiB      0.0 MiB           run_info['early_stop_rounds'] = early_stop_rounds
   317                             
   318    170.1 MiB      0.0 MiB           df_train, n_start = load_data(load_test=False, N_start=N_start, N_read=N_files_train, N_split=N_splits,
   319    170.1 MiB      0.0 MiB                                         shuffle=False,
   320   3479.7 MiB   3309.6 MiB                                         feature_down_select=feature_down_select)
   321   3479.7 MiB      0.0 MiB           df_test, _ = load_data(load_test=True, N_start=n_start, N_read=N_files_test, N_split=N_splits, shuffle=False,
   322   6721.4 MiB   3241.6 MiB                                  feature_down_select=feature_down_select)
   323                             
   324   6721.4 MiB      0.0 MiB           ub.log('generating id diff columns based on various dates columns')
   325   6721.4 MiB      0.0 MiB           dates_cols = [x for x in list(df_train.columns) if 'start_date' in x or 'end_date' in x]
   326                             
   327                                     # print dates_cols
   328                             
   329   8013.2 MiB   1291.8 MiB           df_datesort = pd.concat([df_train[['Id'] + dates_cols], df_test[['Id'] + dates_cols]],
   330   8013.4 MiB      0.2 MiB                                   ignore_index=True)
   331   8013.4 MiB      0.0 MiB           gc.collect()
   332                             
   333  31533.7 MiB  23520.4 MiB           for c in dates_cols:
   334  31579.8 MiB     46.1 MiB               df_datesort.sort_values(by=[c, 'Id'], inplace=True)
   335  31597.6 MiB     17.8 MiB               df_datesort[c + '_id_diff'] = df_datesort['Id'].diff().fillna(999999).astype(int)
   336  29185.6 MiB  -2412.0 MiB               df_datesort[c + '_id_diff_reverse'] = df_datesort['Id'].iloc[::-1].diff().fillna(999999).astype(int)
   337                             
   338  31533.7 MiB   2348.1 MiB               df_datesort.drop([c], axis=1, inplace=True)
   339                             
   340  26244.9 MiB  -5288.8 MiB           df_datesort.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_datesort_debug.csv'))
   341                             
   342   9798.9 MiB -16446.1 MiB           gc.collect()
   343  14233.2 MiB   4434.4 MiB           df_train = df_train.merge(df_datesort, on='Id')
   344  14233.2 MiB      0.0 MiB           df_train.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_train_debug.csv'))
   345                             
   346  11009.0 MiB  -3224.2 MiB           gc.collect()
   347  15443.4 MiB   4434.4 MiB           df_test = df_test.merge(df_datesort, on='Id')
   348  15443.4 MiB      0.0 MiB           df_test.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_test_debug.csv'))
   349                             
   350  15443.4 MiB      0.0 MiB           df_test['Response'] = 0
   351                             
   352  15443.4 MiB      0.0 MiB           print df_train.shape
   353  15443.4 MiB      0.0 MiB           print df_test.shape
   354  12219.2 MiB  -3224.2 MiB           gc.collect()
   355                             
   356  12219.2 MiB      0.0 MiB           if N_files_train == N_splits:
   357  12219.2 MiB      0.0 MiB               split_data(df_train,
   358  12219.2 MiB      0.0 MiB                          output_fname_template=os.path.join(ub.processed_data_dir, 'df_train_preprocessed_part{}.csv'))
   359  12219.2 MiB      0.0 MiB           if N_files_test == N_splits:
   360  12219.2 MiB      0.0 MiB               split_data(df_test,
   361  12219.2 MiB      0.0 MiB                          output_fname_template=os.path.join(ub.processed_data_dir, 'df_test_preprocessed_part{}.csv'))
   362                             
   363  12219.2 MiB      0.0 MiB           fillna = True
   364  12219.2 MiB      0.0 MiB           run_info['fillna'] = fillna
   365  12219.2 MiB      0.0 MiB           if fillna:
   366  12219.2 MiB      0.0 MiB               ub.log('Filling na...')
   367  12417.9 MiB    198.7 MiB               for df in [df_train, df_test]:
   368  12417.9 MiB      0.0 MiB                   cols_full_flag = df.isnull().any()
   369  12417.9 MiB      0.0 MiB                   non_full_cols = list(cols_full_flag[cols_full_flag].index)
   370  12417.9 MiB      0.0 MiB                   print 'Non-full columns: {}'.format(len(non_full_cols))
   371                                             # print non_full_cols
   372                             
   373                                             if 1:
   374  12417.9 MiB      0.0 MiB                       df.fillna(-999999, inplace=True)
   375                                             else:
   376                                                 # print df.PersonalField7.unique()
   377                                                 for c in non_full_cols:
   378                                                     if len(df[c].unique()) > 2:
   379                                                         most_frequent_items = df[c].value_counts().idxmax()
   380                                                         print c, most_frequent_items
   381                                                         df[c].fillna(value=most_frequent_items, inplace=True)
   382                                                     else:  # if it is only a pair of value [somthing, nan] then fill in "missing"
   383                                                         df[c].fillna(value='missing', inplace=True)
   384                                                         print c, df[c].unique()
   385                             
   386  12417.9 MiB      0.0 MiB                   cols_full_flag = df.isnull().any()
   387  12417.9 MiB      0.0 MiB                   non_full_cols = list(cols_full_flag[cols_full_flag].index)
   388  12417.9 MiB      0.0 MiB                   print 'Non-full columns: {}'.format(len(non_full_cols))
   389                             
   390  12417.9 MiB      0.0 MiB                   le = LabelEncoder()
   391  12417.9 MiB      0.0 MiB                   obj_cols = df.select_dtypes(include=['object']).columns
   392                                             # print 'Obj columns: ', list(obj_cols)
   393  12417.9 MiB      0.0 MiB                   for col in obj_cols:
   394  12417.9 MiB      0.0 MiB                       df[col] = le.fit_transform(df[col])
   395                             
   396  12417.9 MiB      0.0 MiB               df_train.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_train_cleanup_debug.csv'))
   397  12417.9 MiB      0.0 MiB               df_test.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_train_cleanup_debug.csv'))
   398                             
   399  12417.9 MiB      0.0 MiB           ub.log('Dropping Id and Response columns...')
   400  12417.9 MiB      0.0 MiB           columns_to_drop = ['Id', 'Response']
   401  12427.0 MiB      9.0 MiB           shuffle_col = df_train[['Id']].copy()
   402  12427.0 MiB      0.0 MiB           shuffle_col['Id'] = np.random.randn(len(shuffle_col))
   403                             
   404  12427.0 MiB      0.0 MiB           y_total_df = df_train['Response']
   405  12427.0 MiB      0.0 MiB           y_total = df_train['Response'].values
   406  13727.5 MiB   1300.5 MiB           df_train.drop(columns_to_drop, axis=1, inplace=True)
   407  13718.4 MiB     -9.0 MiB           df_test.drop(columns_to_drop, axis=1, inplace=True)
   408                             
   409  13718.4 MiB      0.0 MiB           print df_train.shape
   410  13718.4 MiB      0.0 MiB           print df_test.shape
   411  13718.4 MiB      0.0 MiB           prior = np.sum(y_total) / (1. * len(y_total))
   412  13718.4 MiB      0.0 MiB           print 'prior: {}'.format(prior)
   413  13718.4 MiB      0.0 MiB           run_info['prior'] = prior
   414  13718.4 MiB      0.0 MiB           gc.collect()
   415                             
   416  13718.4 MiB      0.0 MiB           feature_imp_fname_tmplate = os.path.join(ub.output_dir, 'feature_importance_xgb_{}')
   417  13718.4 MiB      0.0 MiB           top_features_fname = feature_imp_fname_tmplate.format('accumu_list.txt')
   418                                     # if feature_down_select:
   419                                     #     ub.log('Feature down selected based on {}...'.format(top_features_fname))
   420                                     #     #todo may need to set a maxN for the number of features to use
   421                                     #
   422                                     #     with open(top_features_fname, 'r') as tf:
   423                                     #         selected_cols = [x.strip() for x in tf.readlines()]
   424                                     #     df_train = df_train[selected_cols]
   425                                     #     df_test = df_test[selected_cols]
   426                                     #     print df_train.shape
   427                                     #     print df_test.shape
   428                                     #     print df_train.columns
   429                             
   430  13718.4 MiB      0.0 MiB           feature_names = list(df_train.columns)
   431                             
   432  13718.4 MiB      0.0 MiB           postfix_train = '{}_{}of{}'.format(datetime_str, N_files_train, N_splits)
   433  13718.4 MiB      0.0 MiB           postfix_test = '{}_{}of{}'.format(datetime_str, N_files_test, N_splits)
   434                             
   435  13718.4 MiB      0.0 MiB           run_info['postfix_train'] = postfix_train
   436  13718.4 MiB      0.0 MiB           run_info['postfix_test'] = postfix_test
   437                             
   438  13718.4 MiB      0.0 MiB           testsize = 0.3
   439  13718.4 MiB      0.0 MiB           run_info['testsize'] = testsize
   440                             
   441  13718.4 MiB      0.0 MiB           train_test_split_method = 1
   442                             
   443  13718.4 MiB      0.0 MiB           ub.log('Train/val split using testsize={}, split_method={}'.format(testsize, train_test_split_method))
   444  13718.4 MiB      0.0 MiB           if train_test_split_method == 1:
   445  13718.4 MiB      0.0 MiB               train_idx = shuffle_col[shuffle_col['Id'] > testsize].index
   446  13718.4 MiB      0.0 MiB               val_idx = shuffle_col[shuffle_col['Id'] <= testsize].index
   447  13718.4 MiB      0.0 MiB               ub.log('Done shuffling...')
   448  13718.4 MiB      0.0 MiB               print 'len of train_idx', len(train_idx)
   449  13718.4 MiB      0.0 MiB               print 'len of val_idx', len(val_idx)
   450  13718.4 MiB      0.0 MiB               y_train = y_total_df.loc[train_idx].values
   451  13718.4 MiB      0.0 MiB               y_val = y_total_df.loc[val_idx].values
   452                             
   453  15447.8 MiB   1729.4 MiB               xgtrain = xgb.DMatrix(df_train.loc[train_idx].values, y_train, feature_names=feature_names)
   454  15447.8 MiB      0.0 MiB               ub.log('Assembled xgtrain')
   455  18246.9 MiB   2799.1 MiB               xgval = xgb.DMatrix(df_train.loc[val_idx].values, y_val, feature_names=feature_names)
   456  18246.9 MiB      0.0 MiB               ub.log('Assembled xgval')
   457  18246.9 MiB      0.0 MiB               del df_train
   458  18246.9 MiB      0.0 MiB               ub.log('Deleted df_train')
   459  13722.2 MiB  -4524.7 MiB               gc.collect()
   460                                     else:
   461                                         x_train, x_val, y_train, y_val = train_test_split(df_train.values, y_total, test_size=testsize)
   462                                         ub.log('Done shuffling...')
   463                                         print x_train.shape
   464                                         print x_val.shape
   465                                         del df_train
   466                                         gc.collect()
   467                                         ub.log('Deleted df_train')
   468                             
   469                                         xgtrain = xgb.DMatrix(x_train, y_train, feature_names=feature_names)
   470                                         ub.log('Assembled xgtrain')
   471                                         xgval = xgb.DMatrix(x_val, y_val, feature_names=feature_names)
   472                                         ub.log('Assembled xgval')
   473                                         del x_train
   474                                         del x_val
   475                                         gc.collect()
   476                             
   477  13722.2 MiB      0.0 MiB           fname_xgtrain = os.path.join(ub.processed_data_dir, 'xgtrain_{}.buffer'.format(postfix_train))
   478  13722.3 MiB      0.1 MiB           xgtrain.save_binary(fname_xgtrain)
   479  13722.3 MiB      0.0 MiB           ub.log('Saved {}'.format(fname_xgtrain))
   480                             
   481  13722.3 MiB      0.0 MiB           fname_xgval = os.path.join(ub.processed_data_dir, 'xgval_{}.buffer'.format(postfix_train))
   482  13722.3 MiB      0.0 MiB           xgval.save_binary(fname_xgval)
   483  13722.3 MiB      0.0 MiB           ub.log('Saved {}'.format(fname_xgval))
   484                             
   485  18247.4 MiB   4525.1 MiB           xgtest = xgb.DMatrix(df_test.values, feature_names=feature_names)
   486  18247.4 MiB      0.0 MiB           ub.log('Assembled xgtest')
   487  18247.4 MiB      0.0 MiB           fname_xgtest = os.path.join(ub.processed_data_dir, 'xgtest_{}.buffer'.format(postfix_test))
   488  18247.4 MiB      0.0 MiB           xgtest.save_binary(fname_xgtest)
   489  18247.4 MiB      0.0 MiB           ub.log('Saved {}'.format(fname_xgtest))
   490                             
   491  18247.4 MiB      0.0 MiB           del df_test
   492  18247.4 MiB      0.0 MiB           gc.collect()
   493  18247.4 MiB      0.0 MiB           ub.log('Deleted df_test')
   494                             
   495  18247.4 MiB      0.0 MiB           print 'train and val set sizes'
   496  18247.4 MiB      0.0 MiB           print xgtrain.num_row(), xgtrain.num_col()
   497  18247.4 MiB      0.0 MiB           print xgval.num_row(), xgval.num_col()
   498                             
   499  18247.4 MiB      0.0 MiB           run_info['fname_xgtrain'] = fname_xgtrain
   500  18247.4 MiB      0.0 MiB           run_info['fname_xgval'] = fname_xgval
   501  18247.4 MiB      0.0 MiB           run_info['fname_xgtest'] = fname_xgtest
   502                             
   503  18247.4 MiB      0.0 MiB           xgb_params = get_params(basescore=prior)
   504  18247.4 MiB      0.0 MiB           run_info['xgb_params'] = xgb_params
   505  18247.4 MiB      0.0 MiB           ub.log('Get xgb_params')
   506  18247.4 MiB      0.0 MiB           print xgb_params
   507                             
   508  18247.4 MiB      0.0 MiB           xgb_num_rounds = 2000
   509  18247.4 MiB      0.0 MiB           run_info['xgb_num_rounds'] = xgb_num_rounds
   510  18247.4 MiB      0.0 MiB           print 'xgb_num_rounds', xgb_num_rounds
   511  18247.4 MiB      0.0 MiB           if cv:
   512                                         ub.log('Running cross validation...')
   513                                         eval_hist = xgb.cv(xgb_params, xgtrain, num_boost_round=xgb_num_rounds,
   514                                                            early_stopping_rounds=early_stop_rounds,
   515                                                            feval=ub.mcc_eval, maximize=True,
   516                                                            verbose_eval=1, show_stdv=True, nfold=4, seed=0, stratified=True)
   517                                         print eval_hist
   518                                         run_info['eval_hist'] = eval_hist
   519                             
   520                                     else:
   521  18247.4 MiB      0.0 MiB               ub.log('Running training...')
   522  18247.4 MiB      0.0 MiB               feature_imp_fname = feature_imp_fname_tmplate.format(postfix_train)
   523  18247.4 MiB      0.0 MiB               watchlist = [(xgtrain, 'train'), (xgval, 'eval')]
   524  18247.4 MiB      0.0 MiB               model = xgb.train(xgb_params, xgtrain, num_boost_round=xgb_num_rounds,
   525  18247.4 MiB      0.0 MiB                                 early_stopping_rounds=early_stop_rounds,
   526  18247.4 MiB      0.0 MiB                                 feval=ub.mcc_eval, maximize=True,
   527  19976.5 MiB   1729.2 MiB                                 evals=watchlist, verbose_eval=True)
   528                             
   529  19976.5 MiB      0.0 MiB               model_fname = os.path.join(ub.output_dir, 'xbg_{}.model'.format(postfix_train))
   530  19976.5 MiB      0.0 MiB               ub.log('Saving model: {}...'.format(model_fname))
   531  19976.5 MiB      0.0 MiB               model.save_model(model_fname)
   532  19976.5 MiB      0.0 MiB               model.dump_model(model_fname + '.raw.txt')
   533  19976.5 MiB      0.0 MiB               run_info['model_fname'] = model_fname
   534                             
   535  19976.5 MiB      0.0 MiB               ntree_limit = model.best_iteration + 1
   536                             
   537  19976.5 MiB      0.0 MiB               ub.log('Predictions on xgtrain...', 'highlight')
   538  19976.5 MiB      0.0 MiB               predictions = model.predict(xgtrain, ntree_limit=ntree_limit)
   539                             
   540  19976.5 MiB      0.0 MiB               best_proba, best_mcc, y_pred = ub.eval_mcc(y_train, predictions, True)
   541  19976.5 MiB      0.0 MiB               mcc_official = matthews_corrcoef(y_train, y_pred)
   542  19976.5 MiB      0.0 MiB               print 'ntree limit:', ntree_limit
   543  19976.5 MiB      0.0 MiB               print 'best_mcc:', best_mcc
   544  19976.5 MiB      0.0 MiB               print 'best_proba:', best_proba
   545  19976.5 MiB      0.0 MiB               print 'matthews_corroef', mcc_official
   546                             
   547  19976.5 MiB      0.0 MiB               ub.log('Predictions on xgval...', 'highlight')
   548  19976.5 MiB      0.0 MiB               predictions = model.predict(xgval, ntree_limit=ntree_limit)
   549                             
   550  19976.5 MiB      0.0 MiB               best_proba, best_mcc, y_pred = ub.eval_mcc(y_val, predictions, True)
   551  19976.5 MiB      0.0 MiB               mcc_official = matthews_corrcoef(y_val, y_pred)
   552  19976.5 MiB      0.0 MiB               print 'ntree limit:', ntree_limit
   553  19976.5 MiB      0.0 MiB               print 'best_mcc:', best_mcc
   554  19976.5 MiB      0.0 MiB               print 'best_proba:', best_proba
   555  19976.5 MiB      0.0 MiB               print 'matthews_corroef', mcc_official
   556                             
   557  19976.5 MiB      0.0 MiB               run_info['ntree_limit'] = ntree_limit
   558  19976.5 MiB      0.0 MiB               run_info['best_mcc'] = best_mcc
   559  19976.5 MiB      0.0 MiB               run_info['best_proba'] = best_proba
   560  19976.5 MiB      0.0 MiB               run_info['mcc_official'] = mcc_official
   561                             
   562  19976.5 MiB      0.0 MiB               if analyze_feature_importance:
   563                                             ub.log('Analyzing feature importance...')
   564                                             imp = model.get_fscore()
   565                                             imp = sorted(imp.items(), key=operator.itemgetter(1))
   566                                             imp_df = pd.DataFrame(imp, columns=['feature', 'fscore'])
   567                                             imp_df['fscore'] = imp_df['fscore'] / imp_df['fscore'].sum()
   568                             
   569                                             ub.log('Output result csv to {}...'.format(feature_imp_fname + '.csv'))
   570                                             imp_df.to_csv(feature_imp_fname + '.csv')
   571                             
   572                                             plt.figure()
   573                                             imp_df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))
   574                                             plt.title('XGBoost Feature Importance @ {}'.format(postfix_train))
   575                                             plt.xlabel('relative importance')
   576                                             plt.gcf().savefig(feature_imp_fname + '.png', bbox_inches='tight')
   577                             
   578                                             feature_lists = glob.glob(feature_imp_fname_tmplate.replace('{}', '*.csv'))
   579                                             ub.log('Aggregating previous analysis results...')
   580                                             print feature_lists
   581                                             features_df = None
   582                                             if feature_lists:
   583                                                 for f_l in feature_lists:
   584                                                     tmp_df = pd.read_csv(f_l, index_col=0)
   585                                                     if features_df is None:
   586                                                         features_df = tmp_df
   587                                                     else:
   588                                                         features_df = pd.concat([features_df, tmp_df], ignore_index=True)
   589                             
   590                                             f_df = features_df.groupby(['feature']).mean().reset_index()
   591                                             f_df['overall'] = True
   592                                             imp_df['overall'] = False
   593                                             merged_df = pd.concat([imp_df, f_df]).sort_values(by=['overall', 'fscore'], ascending=False)
   594                                             sns_plot = sns.factorplot(y='feature', x='fscore', data=merged_df, hue='overall', kind='bar',
   595                                                                       hue_order=[True, False], size=20, aspect=0.5)
   596                                             sns_plot.savefig(feature_imp_fname + '_overall.png', bbox_inches='tight')
   597                             
   598                                             ub.log('Output overall result csv to {}...'.format(top_features_fname))
   599                                             with open(top_features_fname, 'w') as tf:
   600                                                 tf.write('\n'.join(list(set(merged_df.feature.values))))
   601                             
   602                                             merged_df.to_csv(top_features_fname.replace('.txt', '_df.csv'), index=False)
   603                             
   604  19976.5 MiB      0.0 MiB           run_info_fname = os.path.join(ub.output_dir, 'run_info_{}.txt'.format(postfix_train))
   605  19976.5 MiB      0.0 MiB           ub.log('Saving run_info into {}'.format(run_info_fname))
   606  19976.5 MiB      0.0 MiB           print run_info
   607  19976.5 MiB      0.0 MiB           with open(run_info_fname, 'w') as fp:
   608  19976.5 MiB      0.0 MiB               fp.write(str(run_info))
   609                             
   610                                         # json has trouble serializing np.float32
   611                                         # with open(run_info_fname, 'w') as fp:
   612                                         #    json.dump(run_info, fp)
   613                                 else:
   614                                     ub.log('Loading run info from {} ...'.format(run_info_fname))
   615                                     with open(run_info_fname, 'r') as fp:
   616                                         run_info = eval(fp.read())
   617                                     print json.dumps(run_info, indent=2)
   618                             
   619                                     model = xgb.Booster()
   620                                     ub.log('Loading model {} ...'.format(run_info['model_fname']))
   621                                     model.load_model(run_info['model_fname'])
   622                                     ub.log('Loading xgtest data {} ...'.format(run_info['fname_xgtest']))
   623                                     xgtest = xgb.DMatrix(run_info['fname_xgtest'])
   624                                     ub.log('XGB making predictions...')
   625                                     ypred = model.predict(xgtest, ntree_limit=run_info['ntree_limit'])
   626                             
   627                                     nrows = len(ypred)
   628                                     postfix_train = run_info['postfix_train']
   629                             
   630                                     sample = pd.read_csv(os.path.join(ub.data_dir, 'sample_submission.csv'), nrows=nrows)
   631                                     sample['Response'] = ypred
   632                                     fname_output = os.path.join(ub.output_dir, "sub_xgboost_{}_prob.csv".format(postfix_train))
   633                                     ub.log('Writing output file (raw proba) {} ...'.format(fname_output))
   634                                     sample.to_csv(fname_output, index=False)
   635                             
   636                                     best_proba = run_info['best_proba']
   637                                     ub.log('Using threshold: best_proba == {}'.format(best_proba))
   638                                     sample['Response'] = (ypred > best_proba).astype(int)
   639                                     fname_output = os.path.join(ub.output_dir, "sub_xgboost_{}.csv".format(postfix_train))
   640                                     ub.log('Writing output file {} ...'.format(fname_output))
   641                                     sample.to_csv(fname_output, index=False)
   642  19976.5 MiB      0.0 MiB       return run_info_fname


Filename: explore2.py

Line #    Mem usage    Increment   Line Contents
================================================
   286   9195.8 MiB      0.0 MiB   @profile(stream=f_mem)
   287                             def main(run_info_fname=None,
   288                                      N_start=None,
   289                                      N_files_train=10,
   290                                      N_files_test=10,
   291                                      cv=False,
   292                                      # if True running cross validation if False, run single model training session and importance analysis
   293                                      feature_down_select=False,
   294                                      analyze_feature_importance=False,
   295                                      early_stop_rounds=10
   296                                      ):
   297   9195.8 MiB      0.0 MiB       datetime_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
   298                             
   299   9195.8 MiB      0.0 MiB       if run_info_fname is None:
   300                                     run_info = dict()
   301                                     N_splits = ub.N_split
   302                                     if N_files_train > N_splits:
   303                                         N_files_train = N_splits
   304                                     if N_files_test > N_splits:
   305                                         N_files_test = N_splits
   306                             
   307                                     if analyze_feature_importance and feature_down_select:
   308                                         assert 0
   309                             
   310                                     run_info['N_splits'] = N_splits
   311                                     run_info['N_files_train'] = N_files_train
   312                                     run_info['N_files_test'] = N_files_test
   313                                     run_info['feature_down_select'] = feature_down_select
   314                                     run_info['cv'] = cv
   315                                     run_info['analyze_feature_importance'] = analyze_feature_importance
   316                                     run_info['early_stop_rounds'] = early_stop_rounds
   317                             
   318                                     df_train, n_start = load_data(load_test=False, N_start=N_start, N_read=N_files_train, N_split=N_splits,
   319                                                                   shuffle=False,
   320                                                                   feature_down_select=feature_down_select)
   321                                     df_test, _ = load_data(load_test=True, N_start=n_start, N_read=N_files_test, N_split=N_splits, shuffle=False,
   322                                                            feature_down_select=feature_down_select)
   323                             
   324                                     ub.log('generating id diff columns based on various dates columns')
   325                                     dates_cols = [x for x in list(df_train.columns) if 'start_date' in x or 'end_date' in x]
   326                             
   327                                     # print dates_cols
   328                             
   329                                     df_datesort = pd.concat([df_train[['Id'] + dates_cols], df_test[['Id'] + dates_cols]],
   330                                                             ignore_index=True)
   331                                     gc.collect()
   332                             
   333                                     for c in dates_cols:
   334                                         df_datesort.sort_values(by=[c, 'Id'], inplace=True)
   335                                         df_datesort[c + '_id_diff'] = df_datesort['Id'].diff().fillna(999999).astype(int)
   336                                         df_datesort[c + '_id_diff_reverse'] = df_datesort['Id'].iloc[::-1].diff().fillna(999999).astype(int)
   337                             
   338                                         df_datesort.drop([c], axis=1, inplace=True)
   339                             
   340                                     df_datesort.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_datesort_debug.csv'))
   341                             
   342                                     gc.collect()
   343                                     df_train = df_train.merge(df_datesort, on='Id')
   344                                     df_train.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_train_debug.csv'))
   345                             
   346                                     gc.collect()
   347                                     df_test = df_test.merge(df_datesort, on='Id')
   348                                     df_test.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_test_debug.csv'))
   349                             
   350                                     df_test['Response'] = 0
   351                             
   352                                     print df_train.shape
   353                                     print df_test.shape
   354                                     gc.collect()
   355                             
   356                                     if N_files_train == N_splits:
   357                                         split_data(df_train,
   358                                                    output_fname_template=os.path.join(ub.processed_data_dir, 'df_train_preprocessed_part{}.csv'))
   359                                     if N_files_test == N_splits:
   360                                         split_data(df_test,
   361                                                    output_fname_template=os.path.join(ub.processed_data_dir, 'df_test_preprocessed_part{}.csv'))
   362                             
   363                                     fillna = True
   364                                     run_info['fillna'] = fillna
   365                                     if fillna:
   366                                         ub.log('Filling na...')
   367                                         for df in [df_train, df_test]:
   368                                             cols_full_flag = df.isnull().any()
   369                                             non_full_cols = list(cols_full_flag[cols_full_flag].index)
   370                                             print 'Non-full columns: {}'.format(len(non_full_cols))
   371                                             # print non_full_cols
   372                             
   373                                             if 1:
   374                                                 df.fillna(-999999, inplace=True)
   375                                             else:
   376                                                 # print df.PersonalField7.unique()
   377                                                 for c in non_full_cols:
   378                                                     if len(df[c].unique()) > 2:
   379                                                         most_frequent_items = df[c].value_counts().idxmax()
   380                                                         print c, most_frequent_items
   381                                                         df[c].fillna(value=most_frequent_items, inplace=True)
   382                                                     else:  # if it is only a pair of value [somthing, nan] then fill in "missing"
   383                                                         df[c].fillna(value='missing', inplace=True)
   384                                                         print c, df[c].unique()
   385                             
   386                                             cols_full_flag = df.isnull().any()
   387                                             non_full_cols = list(cols_full_flag[cols_full_flag].index)
   388                                             print 'Non-full columns: {}'.format(len(non_full_cols))
   389                             
   390                                             le = LabelEncoder()
   391                                             obj_cols = df.select_dtypes(include=['object']).columns
   392                                             # print 'Obj columns: ', list(obj_cols)
   393                                             for col in obj_cols:
   394                                                 df[col] = le.fit_transform(df[col])
   395                             
   396                                         df_train.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_train_cleanup_debug.csv'))
   397                                         df_test.head(n=1000).to_csv(os.path.join(ub.data_dir, 'df_train_cleanup_debug.csv'))
   398                             
   399                                     ub.log('Dropping Id and Response columns...')
   400                                     columns_to_drop = ['Id', 'Response']
   401                                     shuffle_col = df_train[['Id']].copy()
   402                                     shuffle_col['Id'] = np.random.randn(len(shuffle_col))
   403                             
   404                                     y_total_df = df_train['Response']
   405                                     y_total = df_train['Response'].values
   406                                     df_train.drop(columns_to_drop, axis=1, inplace=True)
   407                                     df_test.drop(columns_to_drop, axis=1, inplace=True)
   408                             
   409                                     print df_train.shape
   410                                     print df_test.shape
   411                                     prior = np.sum(y_total) / (1. * len(y_total))
   412                                     print 'prior: {}'.format(prior)
   413                                     run_info['prior'] = prior
   414                                     gc.collect()
   415                             
   416                                     feature_imp_fname_tmplate = os.path.join(ub.output_dir, 'feature_importance_xgb_{}')
   417                                     top_features_fname = feature_imp_fname_tmplate.format('accumu_list.txt')
   418                                     # if feature_down_select:
   419                                     #     ub.log('Feature down selected based on {}...'.format(top_features_fname))
   420                                     #     #todo may need to set a maxN for the number of features to use
   421                                     #
   422                                     #     with open(top_features_fname, 'r') as tf:
   423                                     #         selected_cols = [x.strip() for x in tf.readlines()]
   424                                     #     df_train = df_train[selected_cols]
   425                                     #     df_test = df_test[selected_cols]
   426                                     #     print df_train.shape
   427                                     #     print df_test.shape
   428                                     #     print df_train.columns
   429                             
   430                                     feature_names = list(df_train.columns)
   431                             
   432                                     postfix_train = '{}_{}of{}'.format(datetime_str, N_files_train, N_splits)
   433                                     postfix_test = '{}_{}of{}'.format(datetime_str, N_files_test, N_splits)
   434                             
   435                                     run_info['postfix_train'] = postfix_train
   436                                     run_info['postfix_test'] = postfix_test
   437                             
   438                                     testsize = 0.3
   439                                     run_info['testsize'] = testsize
   440                             
   441                                     train_test_split_method = 1
   442                             
   443                                     ub.log('Train/val split using testsize={}, split_method={}'.format(testsize, train_test_split_method))
   444                                     if train_test_split_method == 1:
   445                                         train_idx = shuffle_col[shuffle_col['Id'] > testsize].index
   446                                         val_idx = shuffle_col[shuffle_col['Id'] <= testsize].index
   447                                         ub.log('Done shuffling...')
   448                                         print 'len of train_idx', len(train_idx)
   449                                         print 'len of val_idx', len(val_idx)
   450                                         y_train = y_total_df.loc[train_idx].values
   451                                         y_val = y_total_df.loc[val_idx].values
   452                             
   453                                         xgtrain = xgb.DMatrix(df_train.loc[train_idx].values, y_train, feature_names=feature_names)
   454                                         ub.log('Assembled xgtrain')
   455                                         xgval = xgb.DMatrix(df_train.loc[val_idx].values, y_val, feature_names=feature_names)
   456                                         ub.log('Assembled xgval')
   457                                         del df_train
   458                                         ub.log('Deleted df_train')
   459                                         gc.collect()
   460                                     else:
   461                                         x_train, x_val, y_train, y_val = train_test_split(df_train.values, y_total, test_size=testsize)
   462                                         ub.log('Done shuffling...')
   463                                         print x_train.shape
   464                                         print x_val.shape
   465                                         del df_train
   466                                         gc.collect()
   467                                         ub.log('Deleted df_train')
   468                             
   469                                         xgtrain = xgb.DMatrix(x_train, y_train, feature_names=feature_names)
   470                                         ub.log('Assembled xgtrain')
   471                                         xgval = xgb.DMatrix(x_val, y_val, feature_names=feature_names)
   472                                         ub.log('Assembled xgval')
   473                                         del x_train
   474                                         del x_val
   475                                         gc.collect()
   476                             
   477                                     fname_xgtrain = os.path.join(ub.processed_data_dir, 'xgtrain_{}.buffer'.format(postfix_train))
   478                                     xgtrain.save_binary(fname_xgtrain)
   479                                     ub.log('Saved {}'.format(fname_xgtrain))
   480                             
   481                                     fname_xgval = os.path.join(ub.processed_data_dir, 'xgval_{}.buffer'.format(postfix_train))
   482                                     xgval.save_binary(fname_xgval)
   483                                     ub.log('Saved {}'.format(fname_xgval))
   484                             
   485                                     xgtest = xgb.DMatrix(df_test.values, feature_names=feature_names)
   486                                     ub.log('Assembled xgtest')
   487                                     fname_xgtest = os.path.join(ub.processed_data_dir, 'xgtest_{}.buffer'.format(postfix_test))
   488                                     xgtest.save_binary(fname_xgtest)
   489                                     ub.log('Saved {}'.format(fname_xgtest))
   490                             
   491                                     del df_test
   492                                     gc.collect()
   493                                     ub.log('Deleted df_test')
   494                             
   495                                     print 'train and val set sizes'
   496                                     print xgtrain.num_row(), xgtrain.num_col()
   497                                     print xgval.num_row(), xgval.num_col()
   498                             
   499                                     run_info['fname_xgtrain'] = fname_xgtrain
   500                                     run_info['fname_xgval'] = fname_xgval
   501                                     run_info['fname_xgtest'] = fname_xgtest
   502                             
   503                                     xgb_params = get_params(basescore=prior)
   504                                     run_info['xgb_params'] = xgb_params
   505                                     ub.log('Get xgb_params')
   506                                     print xgb_params
   507                             
   508                                     xgb_num_rounds = 2000
   509                                     run_info['xgb_num_rounds'] = xgb_num_rounds
   510                                     print 'xgb_num_rounds', xgb_num_rounds
   511                                     if cv:
   512                                         ub.log('Running cross validation...')
   513                                         eval_hist = xgb.cv(xgb_params, xgtrain, num_boost_round=xgb_num_rounds,
   514                                                            early_stopping_rounds=early_stop_rounds,
   515                                                            feval=ub.mcc_eval, maximize=True,
   516                                                            verbose_eval=1, show_stdv=True, nfold=4, seed=0, stratified=True)
   517                                         print eval_hist
   518                                         run_info['eval_hist'] = eval_hist
   519                             
   520                                     else:
   521                                         ub.log('Running training...')
   522                                         feature_imp_fname = feature_imp_fname_tmplate.format(postfix_train)
   523                                         watchlist = [(xgtrain, 'train'), (xgval, 'eval')]
   524                                         model = xgb.train(xgb_params, xgtrain, num_boost_round=xgb_num_rounds,
   525                                                           early_stopping_rounds=early_stop_rounds,
   526                                                           feval=ub.mcc_eval, maximize=True,
   527                                                           evals=watchlist, verbose_eval=True)
   528                             
   529                                         model_fname = os.path.join(ub.output_dir, 'xbg_{}.model'.format(postfix_train))
   530                                         ub.log('Saving model: {}...'.format(model_fname))
   531                                         model.save_model(model_fname)
   532                                         model.dump_model(model_fname + '.raw.txt')
   533                                         run_info['model_fname'] = model_fname
   534                             
   535                                         ntree_limit = model.best_iteration + 1
   536                             
   537                                         ub.log('Predictions on xgtrain...', 'highlight')
   538                                         predictions = model.predict(xgtrain, ntree_limit=ntree_limit)
   539                             
   540                                         best_proba, best_mcc, y_pred = ub.eval_mcc(y_train, predictions, True)
   541                                         mcc_official = matthews_corrcoef(y_train, y_pred)
   542                                         print 'ntree limit:', ntree_limit
   543                                         print 'best_mcc:', best_mcc
   544                                         print 'best_proba:', best_proba
   545                                         print 'matthews_corroef', mcc_official
   546                             
   547                                         ub.log('Predictions on xgval...', 'highlight')
   548                                         predictions = model.predict(xgval, ntree_limit=ntree_limit)
   549                             
   550                                         best_proba, best_mcc, y_pred = ub.eval_mcc(y_val, predictions, True)
   551                                         mcc_official = matthews_corrcoef(y_val, y_pred)
   552                                         print 'ntree limit:', ntree_limit
   553                                         print 'best_mcc:', best_mcc
   554                                         print 'best_proba:', best_proba
   555                                         print 'matthews_corroef', mcc_official
   556                             
   557                                         run_info['ntree_limit'] = ntree_limit
   558                                         run_info['best_mcc'] = best_mcc
   559                                         run_info['best_proba'] = best_proba
   560                                         run_info['mcc_official'] = mcc_official
   561                             
   562                                         if analyze_feature_importance:
   563                                             ub.log('Analyzing feature importance...')
   564                                             imp = model.get_fscore()
   565                                             imp = sorted(imp.items(), key=operator.itemgetter(1))
   566                                             imp_df = pd.DataFrame(imp, columns=['feature', 'fscore'])
   567                                             imp_df['fscore'] = imp_df['fscore'] / imp_df['fscore'].sum()
   568                             
   569                                             ub.log('Output result csv to {}...'.format(feature_imp_fname + '.csv'))
   570                                             imp_df.to_csv(feature_imp_fname + '.csv')
   571                             
   572                                             plt.figure()
   573                                             imp_df.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))
   574                                             plt.title('XGBoost Feature Importance @ {}'.format(postfix_train))
   575                                             plt.xlabel('relative importance')
   576                                             plt.gcf().savefig(feature_imp_fname + '.png', bbox_inches='tight')
   577                             
   578                                             feature_lists = glob.glob(feature_imp_fname_tmplate.replace('{}', '*.csv'))
   579                                             ub.log('Aggregating previous analysis results...')
   580                                             print feature_lists
   581                                             features_df = None
   582                                             if feature_lists:
   583                                                 for f_l in feature_lists:
   584                                                     tmp_df = pd.read_csv(f_l, index_col=0)
   585                                                     if features_df is None:
   586                                                         features_df = tmp_df
   587                                                     else:
   588                                                         features_df = pd.concat([features_df, tmp_df], ignore_index=True)
   589                             
   590                                             f_df = features_df.groupby(['feature']).mean().reset_index()
   591                                             f_df['overall'] = True
   592                                             imp_df['overall'] = False
   593                                             merged_df = pd.concat([imp_df, f_df]).sort_values(by=['overall', 'fscore'], ascending=False)
   594                                             sns_plot = sns.factorplot(y='feature', x='fscore', data=merged_df, hue='overall', kind='bar',
   595                                                                       hue_order=[True, False], size=20, aspect=0.5)
   596                                             sns_plot.savefig(feature_imp_fname + '_overall.png', bbox_inches='tight')
   597                             
   598                                             ub.log('Output overall result csv to {}...'.format(top_features_fname))
   599                                             with open(top_features_fname, 'w') as tf:
   600                                                 tf.write('\n'.join(list(set(merged_df.feature.values))))
   601                             
   602                                             merged_df.to_csv(top_features_fname.replace('.txt', '_df.csv'), index=False)
   603                             
   604                                     run_info_fname = os.path.join(ub.output_dir, 'run_info_{}.txt'.format(postfix_train))
   605                                     ub.log('Saving run_info into {}'.format(run_info_fname))
   606                                     print run_info
   607                                     with open(run_info_fname, 'w') as fp:
   608                                         fp.write(str(run_info))
   609                             
   610                                         # json has trouble serializing np.float32
   611                                         # with open(run_info_fname, 'w') as fp:
   612                                         #    json.dump(run_info, fp)
   613                                 else:
   614   9195.8 MiB      0.0 MiB           ub.log('Loading run info from {} ...'.format(run_info_fname))
   615   9195.8 MiB      0.0 MiB           with open(run_info_fname, 'r') as fp:
   616   9195.8 MiB      0.0 MiB               run_info = eval(fp.read())
   617   9195.8 MiB      0.0 MiB           print json.dumps(run_info, indent=2)
   618                             
   619   9195.8 MiB      0.0 MiB           model = xgb.Booster()
   620   9195.8 MiB      0.0 MiB           ub.log('Loading model {} ...'.format(run_info['model_fname']))
   621   9195.8 MiB      0.0 MiB           model.load_model(run_info['model_fname'])
   622   9195.8 MiB      0.0 MiB           ub.log('Loading xgtest data {} ...'.format(run_info['fname_xgtest']))
   623  13720.3 MiB   4524.5 MiB           xgtest = xgb.DMatrix(run_info['fname_xgtest'])
   624  13720.3 MiB      0.0 MiB           ub.log('XGB making predictions...')
   625  13720.3 MiB      0.0 MiB           ypred = model.predict(xgtest, ntree_limit=run_info['ntree_limit'])
   626                             
   627  13720.3 MiB      0.0 MiB           nrows = len(ypred)
   628  13720.3 MiB      0.0 MiB           postfix_train = run_info['postfix_train']
   629                             
   630  13720.3 MiB      0.0 MiB           sample = pd.read_csv(os.path.join(ub.data_dir, 'sample_submission.csv'), nrows=nrows)
   631  13720.3 MiB      0.0 MiB           sample['Response'] = ypred
   632  13720.3 MiB      0.0 MiB           fname_output = os.path.join(ub.output_dir, "sub_xgboost_{}_prob.csv".format(postfix_train))
   633  13720.3 MiB      0.0 MiB           ub.log('Writing output file (raw proba) {} ...'.format(fname_output))
   634  13720.3 MiB      0.0 MiB           sample.to_csv(fname_output, index=False)
   635                             
   636  13720.3 MiB      0.0 MiB           best_proba = run_info['best_proba']
   637  13720.3 MiB      0.0 MiB           ub.log('Using threshold: best_proba == {}'.format(best_proba))
   638  13720.3 MiB      0.0 MiB           sample['Response'] = (ypred > best_proba).astype(int)
   639  13720.3 MiB      0.0 MiB           fname_output = os.path.join(ub.output_dir, "sub_xgboost_{}.csv".format(postfix_train))
   640  13720.3 MiB      0.0 MiB           ub.log('Writing output file {} ...'.format(fname_output))
   641  13720.3 MiB      0.0 MiB           sample.to_csv(fname_output, index=False)
   642  13720.3 MiB      0.0 MiB       return run_info_fname


